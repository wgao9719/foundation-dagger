model:
  target: algorithms.mineworld.lvm.LlamaLVM
  params:
    model_class: algorithms.mineworld.lvm.LlamaForCausalLM
    tokenizer_config:
      target: algorithms.mineworld.vae.VAE
      params:
        config_path: checkpoints/vae/config.json
        ckpt_path: checkpoints/vae/vae.ckpt
    transformer_config:
      target: transformers.LlamaConfig
      params:
        max_position_embeddings: 5552
        hidden_size: 2048
        intermediate_size: 4096
        num_attention_heads: 32
        num_key_value_heads: 4
        num_hidden_layers: 20
        rope_theta: 10000.0
        torch_dtype: bfloat16
        rms_norm_eps: 1.0e-05
        vocab_size: 8262
        attention_bias: false
        mlp_bias: false

checkpoint: null
device: cuda
token_per_image: 336
action_vocab_offset: 8192
