from typing import List, Tuple, Literal, Optional, Callable, Dict, Any
from collections import defaultdict
import os
import io
from PIL import Image
from omegaconf import DictConfig, OmegaConf
from lightning.pytorch.loggers.logger import Logger
import torch
import matplotlib.pyplot as plt
from einops import repeat, rearrange, reduce, einsum
import wandb
import uuid
from utils.print_utils import cyan


ALL = "all"
ALLType = Literal["all"]
FreqRanges = List[Tuple[float, float] | ALLType]


class HistorySegment:
    r"""
    A class representing a single segment of history.
    Given a 2D grid that decomposes the history in terms of time and frequency (noise level),
    you choose a set of patches in the grid to use guidance on.


    Attributes
    ----------
    time_indices: List[int] | ALLType
        A list of chosen token indices (zero-indexed) along the time axis.
        Python-style negative indices are supported.
        The tokens that are not chosen are completely masked.
        If ALL, all tokens are chosen.
    freq_ranges: List[Tuple[float, float] | ALLType]
        A list of chosen frequency ranges along the frequency axis, for each chosen token.
        One of the following:
        1. Length = len(time_indices): specify frequency ranges for each token
        2. Length = 2: Linearly interpolate between the two frequency ranges for all chosen tokens
            interpolate only within the chosen tokens, i.e. not by global indices
            if len(time_indices) = 1, use the last (second) frequency range
        3. Length = 1: Use the same frequency range for all chosen tokens
        Each element of the list follows:
        - A tuple of (k, k') means the frequency range from noise level k to k' (0 <= k <= k' <= 1).
        - ALL is identical to (0.0, 1.0).
    freq_ranges_if_generated: Optional[List[Tuple[float, float]]]
        For tokens that are generated by the diffusion model instead of
        being ground truth provided by the dataset,
        these frequency ranges are used instead of `freq_ranges`.
        If None, `freq_ranges` is used for all tokens.

    NOTE: The corresponding score for this segment is:
        \nabla_{x} \log p(x | (h[time_indices]^{freq_ranges_start}) - \nabla_{x} \log p(x | h[time_indices]^{freq_ranges_end})
    """

    def __init__(
        self,
        time_indices: List[int] | ALLType = ALL,
        freq_ranges: Optional[FreqRanges] = None,
        freq_ranges_if_generated: Optional[FreqRanges] = None,
    ):
        """Initialize the history segment, see the class docstring for more details."""
        self.time_indices = time_indices
        self.freq_ranges = freq_ranges if freq_ranges is not None else [ALL]
        self.freq_ranges_if_generated = (
            self.freq_ranges
            if freq_ranges_if_generated is None
            else freq_ranges_if_generated
        )

    def _process_freq_ranges(
        self, freq_ranges: FreqRanges, len_chosen: int
    ) -> List[Tuple[float, float]]:
        """
        Process the frequency ranges,
        replacing ALL with (0.0, 1.0), and following rules #1, #2, #3.
        """
        freq_ranges = [
            freq_range if freq_range != ALL else (0.0, 1.0)
            for freq_range in freq_ranges
        ]
        # Case #1
        if len(freq_ranges) == len_chosen:
            return freq_ranges
        # Case #2
        if len(freq_ranges) == 2:
            if len_chosen == 1:
                return [freq_ranges[1]]
            first_start, first_end = freq_ranges[0]
            last_start, last_end = freq_ranges[1]
            return [
                (
                    first_start + (last_start - first_start) * t / (len_chosen - 1),
                    first_end + (last_end - first_end) * t / (len_chosen - 1),
                )
                for t in range(len_chosen)
            ]
        # Case #3
        if len(freq_ranges) == 1:
            return freq_ranges * len_chosen
        raise ValueError(
            f"The length of the history is {len_chosen}, but the length of freq_ranges is {len(freq_ranges)}."
        )

    def to_noise_levels(
        self, hist_mask: torch.Tensor
    ) -> Tuple[Tuple[float], Tuple[float]]:
        """
        Convert the history segment to two partial history conditions,
        whose conditional scores are subtracted to get the composed score.

        Args
        ----
        hist_mask: torch.Tensor, Shape (hist_len,)
            A tensor representing the history mask.
            True means the token is generated and False means the token is ground truth.

        Returns
        -------
        noise_levels: Tuple[Tuple[float], Tuple[float]]:
            A tuple of start and end noise levels for the history segment. Each of length `hist_len`.
            freq_ranges_if_generated or freq_ranges are used, depending on whether the token is generated.
        """
        hist_len = hist_mask.size(0)
        generated_indices = torch.where(hist_mask)[0].tolist()
        time_indices: List[int] = (
            self.time_indices if self.time_indices != ALL else list(range(hist_len))
        )
        assert all(
            t < hist_len for t in time_indices
        ), "time_indices should be between 0 and hist_len."
        time_indices = [
            t if t >= 0 else hist_len + t for t in time_indices
        ]  # handle negative indices
        freq_ranges: List[Tuple[float, float]] = self._process_freq_ranges(
            self.freq_ranges, len(time_indices)
        )
        freq_ranges_if_generated: List[Tuple[float, float]] = self._process_freq_ranges(
            self.freq_ranges_if_generated, len(time_indices)
        )
        final_freq_ranges = [(1.0, 1.0)] * hist_len
        for i, t in enumerate(time_indices):
            final_freq_ranges[t] = (
                freq_ranges_if_generated[i]
                if t in generated_indices
                else freq_ranges[i]
            )

        return tuple(zip(*final_freq_ranges)) if hist_len > 0 else ((), ())

    @classmethod
    def full(cls) -> "HistorySegment":
        """A history segment that contains the full history."""
        return cls(time_indices=ALL, freq_ranges=[ALL])

    @classmethod
    def partial_constant(cls, start_freq: float, end_freq: float) -> "HistorySegment":
        """A history segment that contains a constant frequency range for all tokens."""
        return cls(time_indices=ALL, freq_ranges=[(start_freq, end_freq)])

    @classmethod
    def partial_linear(
        cls, first_range: Tuple[float, float], last_range: Tuple[float, float]
    ) -> "HistorySegment":
        """A history segment that contains a frequency range that changes linearly over time."""
        return cls(time_indices=ALL, freq_ranges=[first_range, last_range])


class HistoryGuidanceVisualizer:
    """
    A visualizer that visualizes the sampling process of history guidance.
    """

    fig: plt.Figure
    axs: List[plt.Axes]
    seq_len: int
    hist_indices: List[int]
    gen_indices: List[int]
    gen_segments: List[List[int]]
    num_gen_segments: int

    log_count = 0  # static variable to only log once

    def __init__(self, num_hist_segments: int, disabled: bool = False):
        self.curr_step = -1
        self.num_hist_segments = num_hist_segments
        self.disabled = disabled or HistoryGuidanceVisualizer.log_count > 0
        self.images = []

    def reset(
        self,
        seq_len: int,
        hist_indices: List[int],
        gen_indices: List[int],
        gen_segments: List[List[int]],
    ):
        if self.disabled:
            return
        self.curr_step += 1
        self.seq_len = seq_len
        self.hist_indices = hist_indices
        self.gen_indices = gen_indices
        self.gen_segments = gen_segments
        self.num_gen_segments = len(gen_segments)
        self.fig, self.axs = plt.subplots(
            self.num_gen_segments,
            self.num_hist_segments,
            figsize=(
                self.seq_len * self.num_hist_segments / 2.0,
                3 * len(self.gen_segments),
            ),
        )
        if self.num_gen_segments == 1:
            self.axs = [self.axs]
        if self.num_hist_segments == 1:
            self.axs = [[ax] for ax in self.axs]

    def add_segment(
        self,
        hist_segment_idx: int,
        noise_levels_start: Tuple[float],
        noise_levels_end: Tuple[float],
        weight: float,
    ):
        if self.disabled:
            return
        for gen_segment_idx in range(len(self.gen_segments)):
            self._add_segment(
                hist_segment_idx,
                gen_segment_idx,
                noise_levels_start,
                noise_levels_end,
                weight,
            )

    def _add_segment(
        self,
        hist_segment_idx: int,
        gen_segment_idx: int,
        noise_levels_start: Tuple[float],
        noise_levels_end: Tuple[float],
        weight: float,
    ):
        ax = self.axs[gen_segment_idx][hist_segment_idx]
        gen_segment = self.gen_segments[gen_segment_idx]
        ax.add_patch(plt.Rectangle((0, 0), self.seq_len, 1, color="grey", alpha=0.15))
        for t, start, end in zip(
            self.hist_indices, noise_levels_start, noise_levels_end
        ):
            # lightgrey
            ax.add_patch(plt.Rectangle((t, 0), 1, 1, color="green", alpha=0.2))
            ax.add_patch(
                plt.Rectangle(
                    (t, start),
                    1,
                    end - start,
                    color="green",
                )
            )
        for i, t in enumerate(self.gen_indices):
            ax.add_patch(
                plt.Rectangle(
                    (t, 0),
                    1,
                    1,
                    color="orange",
                    alpha=1.0 if i in gen_segment else 0.2,
                )
            )
        ax.set_xlim(0, self.seq_len)
        ax.set_ylim(0, 1)
        ax.set_xticks(range(self.seq_len))
        ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_aspect(5)
        ax.grid(which="both", linestyle="--", color="black", alpha=0.75)
        ax.set_xticklabels([])
        ax.set_yticklabels([])
        ax.xaxis.set_ticks_position("none")
        ax.yaxis.set_ticks_position("none")
        ax.set_title(rf"{weight:.1f}$\times$", fontweight="bold")

    def save_frame(self):
        if self.disabled:
            return
        self.fig.suptitle(f"Step {self.curr_step}")
        self.axs[0][0].set_ylabel(r"↑$k$" + "\n" + r"↓$f$", loc="center", rotation=0)
        self.axs[0][0].yaxis.set_label_coords(-0.05, 0.42)

        plt.tight_layout()
        buf = io.BytesIO()
        plt.savefig(buf, format="png")
        self.images.append(Image.open(buf))
        plt.close()

    def to_gif(self) -> io.BytesIO:
        buf = io.BytesIO()
        self.images[0].save(
            buf,
            format="GIF",
            save_all=True,
            append_images=self.images[1:],
            fps=20,
            loop=0,
        )
        buf.seek(0)
        self.disabled = True
        HistoryGuidanceVisualizer.log_count += 1
        return buf


class HistoryGuidanceManager:
    """
    A context manager that actually applies the history guidance to the sampling process.
    """

    mask: torch.Tensor  # Shape (batch_size, seq_len), mask for the entire sequence
    device: torch.device  # Device where computations are done
    hist_indices: torch.Tensor  # Shape (hist_len), indices of history tokens
    gen_indices: torch.Tensor  # Shape (gen_len), indices of tokens to be generated
    num_gen: int  # Number of gen_segments
    num_hist: int  # Number of history conditions to actually be computed
    # NOTE: Number of Function Evaluations (NFE) = num_gen * num_hist
    hist_noise_levels: (
        torch.Tensor
    )  # Shape (num_hist, hist_len), noise levels for history tokens
    weights: (
        torch.Tensor
    )  # Shape (num_hist), weights for each partial history condition
    cond_mask: torch.Tensor  # Shape (num_hist), mask for external conditioning
    gen_mask: torch.Tensor  # Shape (num_gen, seq_len), mask for gen segments
    gen_but_excluded_mask: (
        torch.Tensor
    )  # Shape (batch_size * num_hist, num_gen, seq_len), mask for excluded gen tokens

    def __init__(self, history_guidance: "HistoryGuidance", mask: torch.Tensor):
        """
        Initialize the history guidance manager.

        Args
        ----
        history_guidance: HistoryGuidance
            The history guidance parent object.
        mask: torch.Tensor, Shape (batch_size, seq_len)
            A mask that represents the state of tokens in the sequence.
            0 = to be generated, 1 = ground truth history, 2 = generated history, -1 = padding
        """
        self.history_guidance = history_guidance
        self.visualizer = history_guidance.visualizer
        self.mask = mask
        self.device = mask.device

    @property
    def nfe(self) -> int:
        """Number of Function Evaluations (NFE) per batch & sampling step."""
        return self.num_gen * self.num_hist

    def __enter__(self):
        """
        Precompute all partial history conditions and corresponding weights needed for sampling.
        While history guidance can be computed by doing a weighted sum of composed scores
        for each history segment, there may exist shared score estimations between different segments, and here we avoid redundant computations.
        """

        # 1. Precompute indices and lengths
        reduced_mask = self.mask[0]
        assert (
            self.mask == reduced_mask
        ).all(), "`mask` should be the same across the batch to use history guidance."
        self.hist_indices = torch.where(reduced_mask >= 1)[0]
        self.gen_indices = torch.where(reduced_mask == 0)[0]
        seq_len, hist_len, gen_len = (
            len(reduced_mask),
            len(self.hist_indices),
            len(self.gen_indices),
        )

        # 2. Precompute gen_mask
        gen_segments = [
            gen_segment if gen_segment != ALL else list(range(gen_len))
            for gen_segment in self.history_guidance.gen_segments
        ]
        self.num_gen = len(gen_segments)
        gen_mask = torch.zeros(
            (self.num_gen, seq_len), dtype=torch.bool, device=self.device
        )
        for i, gen_segment in enumerate(gen_segments):
            gen_mask[i, self.gen_indices[gen_segment]] = True
        self.gen_mask = gen_mask

        # 3. Build a dictionary that maps partial history conditions to cumulative weights
        # key = noise level sequence for the history + whether to mask external conditioning
        hist_to_weights: Dict[tuple, float] = defaultdict(float)

        hist_to_weights[
            (1.0,) * hist_len + (self.history_guidance.use_external_cond_guidance,)
        ] = 1.0  # unconditional score

        self.visualizer.reset(
            seq_len, self.hist_indices.tolist(), self.gen_indices.tolist(), gen_segments
        )

        for idx, (hist_segment, weight) in enumerate(
            zip(self.history_guidance.hist_segments, self.history_guidance.hist_weights)
        ):
            noise_levels_start, noise_levels_end = hist_segment.to_noise_levels(
                reduced_mask[self.hist_indices] == 2
            )
            hist_to_weights[noise_levels_start + (False,)] += weight
            hist_to_weights[
                noise_levels_end + (self.history_guidance.use_external_cond_guidance,)
            ] -= weight
            self.visualizer.add_segment(
                idx, noise_levels_start, noise_levels_end, weight
            )

        self.visualizer.save_frame()

        # 4. Convert the dictionary to noise levels, external conditioning mask, and weights
        hist_noise_levels = []
        cond_mask = []
        weights = []
        for hist_cond, weight in hist_to_weights.items():
            if weight == 0:
                continue
            hist_noise_levels.append(hist_cond[:-1])
            cond_mask.append(hist_cond[-1])
            weights.append(weight)
        self.hist_noise_levels = (
            torch.tensor(hist_noise_levels, device=self.device)
            * self.history_guidance.timesteps
            - 1
        ).long()
        self.cond_mask = torch.tensor(cond_mask, device=self.device)
        self.weights = torch.tensor(weights, device=self.device).float()
        self.num_hist = len(self.weights)

        return self

    def __exit__(self, exc_type, exc_value, traceback):
        pass

    def _extend(self, a: torch.Tensor, x: torch.Tensor):
        """Extend the tensor a by adding dimensions at the end to match the shape of x."""
        return rearrange(a, "... -> ..." + " 1" * (x.ndim - a.ndim))

    def prepare(
        self,
        x: torch.Tensor,
        from_noise_levels: torch.Tensor,
        to_noise_levels: torch.Tensor,
        replacement_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
        replacement_only: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Modify the input before passing it to the sampling process.

        Args
        ----
        x: torch.Tensor, Shape (batch_size, seq_len, ...)
            The input tensor to be modified.
        from_noise_levels: torch.Tensor, Shape (batch_size, seq_len)
            Current noise levels of the sequences.
        to_noise_levels: torch.Tensor, Shape (batch_size, seq_len)
            Target noise levels of the sequences.
        replacement_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]
            A function that takes a batch of sequences and replaces clean tokens with noisy tokens,
            depending on the noise levels.
        replacement_only: bool
            If True, only replace clean tokens with noisy tokens, without modifying the noise levels.


        x:
            repeat batch size by NFE, and
            replace history tokens with noisy history tokens depending on the noise levels.
        from_noise_levels, to_noise_levels:
            repeat batch size by NFE, and
            modify the noise levels of the history and tokens to be generated properly.
        cond_mask:
            create a mask for external conditioning.
        """
        b, g, h = x.size(0), self.num_gen, self.num_hist
        x, from_noise_levels, to_noise_levels, mask = map(
            lambda y: repeat(
                y,
                "b t ...-> b h t ...",
                h=h,
            ).clone(),
            (x, from_noise_levels, to_noise_levels, self.mask),
        )

        # 1. Modify the noise levels of the history tokens
        if not replacement_only:
            from_noise_levels[:, :, self.hist_indices] = self.hist_noise_levels
            to_noise_levels[:, :, self.hist_indices] = self.hist_noise_levels

        # 2. Replace clean history tokens with noisy history tokens depending of `from_noise_levels`
        replace_mask = torch.logical_and(from_noise_levels >= 0, mask >= 1)
        x = torch.where(
            self._extend(replace_mask, x),
            rearrange(
                replacement_fn(
                    rearrange(x, "b h t ... -> (b h) t ..."),
                    rearrange(from_noise_levels, "b h t -> (b h) t"),
                ),
                "(b h) t ... -> b h t ...",
                h=h,
            ),
            x,
        )

        x, from_noise_levels, to_noise_levels, mask = map(
            lambda y: repeat(y, "b h t ... -> (b h) g t ...", g=g).clone(),
            (x, from_noise_levels, to_noise_levels, mask),
        )

        # 3. Modify the noise levels of the generated tokens
        # true for tokens to be generated but excluded in a gen segment
        self.gen_but_excluded_mask = torch.logical_and(~self.gen_mask, mask == 0)
        from_noise_levels, to_noise_levels = map(
            lambda y: torch.where(
                self.gen_but_excluded_mask, self.history_guidance.timesteps - 1, y
            ),
            (from_noise_levels, to_noise_levels),
        )

        # 4. Replace tokens to be generated with random noise if excluded
        x = torch.where(
            self._extend(self.gen_but_excluded_mask, x),
            torch.randn_like(x),
            x,
        )

        x, from_noise_levels, to_noise_levels = map(
            lambda y: rearrange(y, "(b h) g t ... -> (b h g) t ...", h=h),
            (x, from_noise_levels, to_noise_levels),
        )

        return (
            x,
            from_noise_levels,
            to_noise_levels,
            repeat(self.cond_mask, "h -> (b h g)", b=b, g=g).clone(),
        )

    def compose(self, x: torch.Tensor) -> torch.Tensor:
        """
        Compose the scores from different history segments using the weights.
        """
        x = rearrange(
            x, "(b h g) t ... -> (b h) g t ...", h=self.num_hist, g=self.num_gen
        )
        # 1. set excluded gen tokens to zero
        x = torch.where(
            self._extend(self.gen_but_excluded_mask, x),
            torch.zeros_like(x),
            x,
        )
        x = rearrange(x, "(b h) g t ... -> b h g t ...", h=self.num_hist)
        # 2. compose using weights
        x = einsum(x, self.weights, "b h g t ..., h -> b g t ...")
        # 3. average across gen segments
        x = reduce(x, "b g t ... -> b t ...", "sum")
        gen_mask_sum = rearrange(
            reduce(self.gen_mask.long(), "g t -> t", "sum").clamp(min=1),
            "t -> t" + " 1" * (x.ndim - 2),
        )
        x /= gen_mask_sum
        return x


class HistoryGuidance:
    r"""
    A generic class for configuring history guidance to Diffusion Forcing 2's sampling process.
    History guidance samples using composed scores from different history segments.

    Attributes
    ----------
    hist_segments: List[HistorySegment]
        The history segments that are composed to form the history guidance.
    hist_weights: List[float]
        The weights assigned to each history segment. For usual cases, each weight should be positive.
    gen_segments: List[List[int] | ALLType]
        Each segment is a list of time indices for the generated tokens that the diffusion model will predict.
    timesteps: int
        The number of timesteps in the discrete-time diffusion process.
    use_external_cond_guidance: bool
        Whether to also use guidance for external conditioning.


    NOTE: Mathematically, the composed score is:
        \token_avg_{gen_segment \in gen_segments} {
            \nabla_{x[gen_segment]} \log p(x[gen_segment] | null) +
            \sum_{hist_segment \in hist_segments} {
               weight_{hist_segment} * \nabla_{x[gen_segment]} \log p(x[gen_segment] | h[hist_segment])
            }
        }
        where the first term is the unconditional score
        and the rest are the weighted sum of scores conditioned on history segments.
    """

    def __init__(
        self,
        hist_segments: List[HistorySegment],
        hist_weights: List[float],
        gen_segments: Optional[List[List[int] | ALLType]] = None,
        timesteps: int = 1000,
        use_external_cond_guidance: bool = False,
        visualize: bool = True,
    ):
        """Initialize the history guidance."""
        self.hist_segments = hist_segments
        self.hist_weights = hist_weights
        if gen_segments is None:
            gen_segments = [ALL]
        self.gen_segments = gen_segments
        self.timesteps = timesteps
        self.use_external_cond_guidance = use_external_cond_guidance
        self.visualizer = HistoryGuidanceVisualizer(
            len(hist_segments), disabled=not visualize
        )
        self._check_attributes()

    def _check_attributes(self):
        """Check if the attributes are valid."""
        assert len(self.hist_segments) == len(
            self.hist_weights
        ), f"Length of hist_segments and hist_weights should be the same, \
        but got {len(self.hist_segments)} and {len(self.hist_weights)}."
        assert (
            len(self.gen_segments) > 0
        ), "At least one gen_segment should be provided."

    def __call__(self, mask: torch.Tensor) -> HistoryGuidanceManager:
        """
        Initialize the history guidance manager.
        This should be called at every sampling step.

        Args
        ----
        mask: torch.Tensor, Shape (batch_size, seq_len)
            A mask that represents the state of tokens in the sequence.
            0 = to be generated, 1 = ground truth history, 2 = generated history, -1 = padding
        """
        return (
            SimpleHistoryGuidanceManager(self, mask)
            if len(self.hist_weights) == 1
            and len(self.hist_segments[0].freq_ranges) == 1
            and self.hist_segments[0].freq_ranges[0] == ALL
            else HistoryGuidanceManager(self, mask)
        )

    def log(self, logger: Optional[Logger] = None):
        """
        Log the history guidance visualization to wandb if logger is provided,
        or save the visualization as a gif otherwise.
        """
        if self.visualizer.disabled:
            return
        gif = self.visualizer.to_gif()
        if logger is None:
            path = f"outputs/history_guidance/{uuid.uuid4()}.gif"
            if not os.path.exists("outputs/history_guidance"):
                os.makedirs("outputs/history_guidance")
            with open(path, "wb") as f:
                f.write(gif.read())
            print(cyan("\nView the history guidance visualization at:") + f"{path}")
        else:
            logger.experiment.log({"history_guidance": wandb.Video(gif, format="gif")})

    @classmethod
    def from_config(
        cls, config: DictConfig, timesteps: int = 1000
    ) -> "HistoryGuidance":
        """
        Initialize the history guidance from a configuration.
        config.name specifies the name of the history guidance scheme,
        and the rest are the arguments for the scheme.
        NOTE: all schemes should use List instead of Tuple as YAML does not support tuples.
        """
        config = OmegaConf.to_container(config, resolve=True)
        name = config.pop("name")
        return getattr(cls, name)(**config, timesteps=timesteps)

    # -----Existing sampling techniques as special cases of history guidance-----

    @classmethod
    def conditional(
        cls, timesteps: int = 1000, visualize: bool = True
    ) -> "HistoryGuidance":
        """
        History guidance scheme equivalent to:
        typical conditional sampling,
        i.e. sampling with only a conditional score (on the entire history).
        """
        return cls(
            hist_segments=[HistorySegment.full()],
            hist_weights=[1],
            timesteps=timesteps,
            use_external_cond_guidance=False,
            visualize=visualize,
        )

    @classmethod
    def stabilized_conditional(
        cls,
        stabilization_level: float,
        timesteps: int = 1000,
        visualize: bool = True,
    ) -> "HistoryGuidance":
        """
        History guidance scheme equivalent to:
        conditional sampling with stabilization technique (Chen et al., https://arxiv.org/abs/2407.01392)
        """
        return cls(
            hist_segments=[
                HistorySegment(
                    time_indices=ALL,
                    freq_ranges=[ALL],
                    freq_ranges_if_generated=[(stabilization_level, 1.0)],
                )
            ],
            hist_weights=[1],
            timesteps=timesteps,
            use_external_cond_guidance=False,
            visualize=visualize,
        )

    @classmethod
    def vanilla(
        cls,
        guidance_scale: float,
        timesteps: int = 1000,
        use_external_cond_guidance: bool = True,
        visualize: bool = True,
    ) -> "HistoryGuidance":
        """
        History guidance scheme equivalent to:
        Vanilla History Guidance (HG-v)
        """
        return cls(
            hist_segments=[HistorySegment.full()],
            hist_weights=[guidance_scale],
            timesteps=timesteps,
            use_external_cond_guidance=use_external_cond_guidance,
            visualize=visualize,
        )

    @classmethod
    def stabilized_vanilla(
        cls,
        guidance_scale: float,
        stabilization_level: float,
        timesteps: int = 1000,
        use_external_cond_guidance: bool = True,
        visualize: bool = True,
    ) -> "HistoryGuidance":
        """
        History guidance scheme equivalent to:
        HG-v with stabilization
        """
        return cls(
            hist_segments=[
                HistorySegment(
                    time_indices=ALL,
                    freq_ranges=[ALL],
                    freq_ranges_if_generated=[(stabilization_level, 1.0)],
                )
            ],
            hist_weights=[guidance_scale],
            timesteps=timesteps,
            use_external_cond_guidance=use_external_cond_guidance,
            visualize=visualize,
        )

    @classmethod
    def fractional(
        cls,
        guidance_scale: float,
        freq_scale: float,
        timesteps: int = 1000,
        use_external_cond_guidance: bool = True,
        visualize: bool = True,
    ) -> "HistoryGuidance":
        """
        History guidance scheme equivalent to:
        Fractional History Guidance (HG-f)
        """
        return cls(
            hist_segments=[
                HistorySegment.full(),
                HistorySegment.partial_constant(freq_scale, 1.0),
            ],
            hist_weights=[1, guidance_scale - 1],
            timesteps=timesteps,
            use_external_cond_guidance=use_external_cond_guidance,
            visualize=visualize,
        )

    @classmethod
    def stabilized_fractional(
        cls,
        guidance_scale: float,
        freq_scale: float,
        stabilization_level: float,
        timesteps: int = 1000,
        use_external_cond_guidance: bool = True,
        visualize: bool = True,
    ) -> "HistoryGuidance":
        """
        History guidance scheme equivalent to:
        HG-f with stabilization
        """
        return cls(
            hist_segments=[
                HistorySegment(
                    time_indices=ALL,
                    freq_ranges=[ALL],
                    freq_ranges_if_generated=[(stabilization_level, 1.0)],
                ),
                HistorySegment.partial_constant(freq_scale, 1.0),
            ],
            hist_weights=[1, guidance_scale - 1],
            timesteps=timesteps,
            use_external_cond_guidance=use_external_cond_guidance,
            visualize=visualize,
        )

    @classmethod
    def temporal(
        cls,
        hist_subsequences: List[List[int] | ALLType],
        hist_weights: List[float],
        gen_segments: Optional[List[List[int] | ALLType]] = None,
        timesteps: int = 1000,
        use_external_cond_guidance: bool = True,
        visualize: bool = True,
    ) -> "HistoryGuidance":
        """
        History guidance scheme equivalent to:
        Temporal History Guidance (HG-t)
        """
        gen_segments = gen_segments if gen_segments is not None else [ALL]
        return cls(
            hist_segments=[
                HistorySegment(time_indices=hist_subsequence)
                for hist_subsequence in hist_subsequences
            ],
            hist_weights=hist_weights,
            gen_segments=gen_segments,
            timesteps=timesteps,
            use_external_cond_guidance=use_external_cond_guidance,
            visualize=visualize,
        )

    @classmethod
    def custom(
        cls,
        hist_segments: List[Dict[str, Any]],
        hist_weights: List[float],
        gen_segments: Optional[List[List[int] | ALLType]] = None,
        timesteps: int = 1000,
        use_external_cond_guidance: bool = True,
        visualize: bool = True,
    ):
        """
        The most flexible way to initialize history guidance.
        """
        gen_segments = gen_segments if gen_segments is not None else [ALL]

        def _list_to_tuple(
            freq_ranges: Optional[List[List[float] | ALLType]],
        ) -> Optional[FreqRanges]:
            if freq_ranges is None:
                return None
            return [
                tuple(freq_range) if freq_range != ALL else ALL
                for freq_range in freq_ranges
            ]

        hist_segments = [
            HistorySegment(
                time_indices=hist_segment["time_indices"],
                freq_ranges=_list_to_tuple(hist_segment["freq_ranges"]),
                freq_ranges_if_generated=_list_to_tuple(
                    hist_segment.get("freq_ranges_if_generated", None)
                ),
            )
            for hist_segment in hist_segments
        ]
        return cls(
            hist_segments=hist_segments,
            hist_weights=hist_weights,
            gen_segments=gen_segments,
            timesteps=timesteps,
            use_external_cond_guidance=use_external_cond_guidance,
            visualize=visualize,
        )


class SimpleHistoryGuidanceManager:
    """
    The implementation of `HistoryGuidanceManager` requires having the same mask across the batch,
    which may not be the case in some applications (e.g. parallel temporal super-resolution).
    `SimpleHistoryGuidanceManager` is a simplified version of `HistoryGuidanceManager`
    that does not require the same mask across the batch, but instead only applicable to
    conditional sampling, or vanilla history guidance (HG-v).
    """

    def __init__(self, history_guidance: "HistoryGuidance", mask: torch.Tensor):
        self.history_guidance = history_guidance
        self.visualizer = history_guidance.visualizer
        self.mask = mask
        self.device = mask.device
        self.guidance_scale = self.history_guidance.hist_weights[0]

    @property
    def nfe(self) -> int:
        return 1 if self.guidance_scale == 1 else 2

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        pass

    def prepare(
        self,
        x: torch.Tensor,
        from_noise_levels: torch.Tensor,
        to_noise_levels: torch.Tensor,
        replacement_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
        replacement_only: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        if self.guidance_scale == 1:
            return x, from_noise_levels, to_noise_levels, None

        x = repeat(x, "b t ... -> b h t ...", h=2).clone()
        from_noise_levels = repeat(from_noise_levels, "b t -> b h t", h=2).clone()
        to_noise_levels = repeat(to_noise_levels, "b t -> b h t", h=2).clone()

        from_noise_levels[:, 0, :] = torch.where(
            self.mask == 1,
            self.history_guidance.timesteps - 1,
            from_noise_levels[:, 0, :],
        )
        to_noise_levels[:, 0, :] = torch.where(
            self.mask == 1,
            self.history_guidance.timesteps - 1,
            to_noise_levels[:, 0, :],
        )

        x[:, 0, :] = torch.where(
            self._extend(self.mask == 1, x[:, 0, :]),
            replacement_fn(x[:, 0, :], from_noise_levels[:, 0, :]),
            x[:, 0, :],
        )
        x, from_noise_levels, to_noise_levels = map(
            lambda y: rearrange(y, "b h t ... -> (b h) t ..."),
            (x, from_noise_levels, to_noise_levels),
        )
        cond_mask = (
            repeat(
                torch.tensor([1, 0], device=self.device).bool(),
                "h -> (b h)",
                b=x.size(0) // 2,
            ).clone()
            if self.history_guidance.use_external_cond_guidance
            else None
        )
        return x, from_noise_levels, to_noise_levels, cond_mask

    def _extend(self, a: torch.Tensor, x: torch.Tensor):
        return rearrange(a, "... -> ..." + " 1" * (x.ndim - a.ndim))

    def compose(self, x: torch.Tensor) -> torch.Tensor:
        if self.guidance_scale == 1:
            return x
        x = rearrange(x, "(b h) t ... -> b h t ...", h=2).clone()
        return x[:, 1, :] * self.guidance_scale - x[:, 0, :] * (self.guidance_scale - 1)
