"""
python -m algorithms.mineworld.mineworld \
  --scene "/Users/willi1/foundation-dagger/diffusion-forcing-transformer/data/mineworld/MineRLBasaltBuildVillageHouse-v0/cheeky-cornflower-setter-0a9ad3ddd136-20220726-194108.mp4" \
  --model_ckpt "/Users/willi1/foundation-dagger/diffusion-forcing-transformer/checkpoints/700M_16f.ckpt" \
  --config "/Users/willi1/foundation-dagger/diffusion-forcing-transformer/configurations/algorithm/mineworld_700M_16f.yaml"
"""


import os
import sys
sys.path.append(os.getcwd())
import gradio as gr
from PIL import Image
import numpy as np
import torch
import cv2
from utils import load_model
from omegaconf import OmegaConf
from argparse import ArgumentParser
from collections import deque
import tempfile
import atexit
from torchvision import transforms
from einops import rearrange
from datasets.mineworld_data.mcdataset import MCDataset
import itertools

class Buttons:
    ATTACK = "attack"
    BACK = "back"
    FORWARD = "forward"
    JUMP = "jump"
    LEFT = "left"
    RIGHT = "right"
    SNEAK = "sneak"
    SPRINT = "sprint"
    USE = "use"
    DROP = "drop"
    SWAPHANDS = "swapHands"
    PICKITEM = "pickItem"

    ALL = [
        ATTACK,
        USE,

        FORWARD,
        BACK,
        LEFT,
        RIGHT,

        JUMP,
        SNEAK,
        SPRINT,

        DROP,
        SWAPHANDS,
        PICKITEM,

        # INVENTORY,
        # ESC,
    ] + [f"hotbar.{i}" for i in range(1, 10)]

KEYBOARD_BUTTON_MAPPING = {
    "key.keyboard.s" :"back",
    "key.keyboard.q" :"drop",
    "key.keyboard.w" :"forward",
    "key.keyboard.1" :"hotbar.1",
    "key.keyboard.2" :"hotbar.2",
    "key.keyboard.3" :"hotbar.3",
    "key.keyboard.4" :"hotbar.4",
    "key.keyboard.5" :"hotbar.5",
    "key.keyboard.6" :"hotbar.6",
    "key.keyboard.7" :"hotbar.7",
    "key.keyboard.8" :"hotbar.8",
    "key.keyboard.9" :"hotbar.9",
    "key.keyboard.space" :"jump",
    "key.keyboard.a" :"left",
    "key.keyboard.d" :"right",
    "key.keyboard.left.shift" :"sneak",
    "key.keyboard.left.control" :"sprint",
    "key.keyboard.f" :"swapHands",
}
# Template action
NOOP_ACTION = {
    "forward": 0,
    "back": 0,
    "left": 0,
    "right": 0,

    "jump": 0,
    "attack": 0,
    "use": 0,
    "pickItem": 0,

    "drop": 0,
    "sneak": 0,
    "sprint": 0,
    "swapHands": 0,

    "hotbar.1": 0,
    "hotbar.2": 0,
    "hotbar.3": 0,
    "hotbar.4": 0,
    "hotbar.5": 0,
    "hotbar.6": 0,
    "hotbar.7": 0,
    "hotbar.8": 0,
    "hotbar.9": 0,
    "camera": np.array([0, 0]),  
}

ACTION_BUTTON = {
    "forward": 0,
    "back": 0,
    "left": 0,
    "right": 0,

    "attack": 0,
    "sprint": 0,
    "jump": 0,
    "use": 0,

    "drop": 0,
    "hotbar.1": 0,
    "pickItem": 0,
}
FOR_BACK = {
    "forward": 0,
    "back": 0,
}
L_R = {
    "left": 0,
    "right": 0,   
}
ATT_USE_DROP = {
    "attack": 0,
    "use": 0,
    "drop": 0,
}
JUMP_SPR = {
    "jump": 0,
    "sprint": 0,
}
HORBAR = {
    "hotbar.1": 0,
    "hotbar.2": 0,
    "hotbar.3": 0,
    "hotbar.4": 0,
    "hotbar.5": 0,
    "hotbar.6": 0,
    "hotbar.7": 0,
    "hotbar.8": 0,
    "hotbar.9": 0,   
}


safe_globals = {"array": np.array}

AGENT_RESOLUTION = (384, 224)
CAMERA_SCALER = 360.0 / 2400.0
TOKEN_PER_IMAGE = 336
TOKEN_PER_ACTION = 11
VIDEO_FRAMES = []
GENERATED_FILES = []
frame_cache = []
action_cache = []
last_pos = 0
MC_ACTION_MAP = MCDataset()
SHOW_FRAMES = 8
REFERENCE_FRAME = None
CONTEXT_LEN = None
DIAGD = False
WINDOWSIZE = 4

def get_args():
    parser = ArgumentParser()
    parser.add_argument('--scene', type=str, default='./assets/scene.mp4')
    parser.add_argument('--model_ckpt', type=str, default='./checkpoints/700M_16f.pt')
    parser.add_argument('--config', type=str, default='./configs/700M_16f.yaml')
    parser.add_argument('--reference_frame', type=int,  default=8)
    parser.add_argument('--diagd', action='store_true', help='use diagd')
    parser.add_argument('--window_size', type=int,  default=4)
    args = parser.parse_args()
    return args

def make_action_dict(action_line):
    action_dict = {'ESC': 0, 'back': 0, 'drop': 0, 'forward': 0, 'hotbar.1': 0, 'hotbar.2': 0, 'hotbar.3': 0, 'hotbar.4': 0, 'hotbar.5': 0, 'hotbar.6': 0, 'hotbar.7': 0, 'hotbar.8': 0, 'hotbar.9': 0, 'inventory': 0, 'jump': 0, 'left': 0, 'right': 0, 'sneak': 0, 'sprint': 0, 'swapHands': 0, 'camera': np.array([0, 0]), 'attack': 0, 'use': 0, 'pickItem': 0}
    if isinstance(action_line, str):
        action_line = action_line.split(",")
        action_dict['camera'] = np.array((int(action_line[-2]), int(action_line[-1])))
    for act in action_line:
        if act in Buttons.ALL:
            action_dict[act] = 1
    
    return action_dict

def stack_images(imgs):
    width, height = imgs[0].size
    new_im = Image.new('RGB', (4*width, height*2))
    for i, im in enumerate(imgs):
        new_im.paste(im, (width*(i%4), height*(i//4)))
    return new_im

def get_action_line(acts):
    action_lst = []
    for k in acts.keys():
        if k != "camera" and acts[k] == 1:
            action_lst.append(k)
    action_lst.append(str(acts["camera"][0]))
    action_lst.append(str(acts["camera"][1]))    

    return ",".join(action_lst)

def run_prediction(btns_choices, cam_x_input, cam_y_input):
    global frame_cache, action_cache, actions_show, images_show, VIDEO_FRAMES, last_pos, CONTEXT_LEN, REFERENCE_FRAME    
    assert len(frame_cache) == len(action_cache)+1
    if len(action_cache) >= CONTEXT_LEN - 1:
        for _ in range(CONTEXT_LEN - REFERENCE_FRAME):
            frame_cache.popleft()
            action_cache.popleft() 
        model.transformer.refresh_kvcache()
        _frame_iter = itertools.islice(frame_cache, 0, len(frame_cache)-1)
        _act_iter = itertools.islice(action_cache, 0, len(action_cache))
        _vis_act = [
                torch.cat([img, act], dim=1)
                for img, act in zip(_frame_iter, _act_iter)
            ]
        _vis_act.append(frame_cache[-1])
        _vis_act = torch.cat(_vis_act, dim=-1)
        _, last_pos = model.transformer.prefill_for_gradio(_vis_act)
        

    act_dict = make_action_dict(btns_choices)
    act_dict['camera'] = np.array((int(cam_y_input), int(cam_x_input)))
    ongoing_act = MC_ACTION_MAP.get_action_index_from_actiondict(act_dict, action_vocab_offset=8192)
    ongoing_act = torch.tensor(ongoing_act).unsqueeze(0).to("cuda")
    action_cache.append(ongoing_act)
    with torch.no_grad(), torch.autocast("cuda", dtype=torch.float16):
        if DIAGD:
            next_frame, last_pos = model.transformer.diagd_img_token_for_gradio(input_action=ongoing_act, position_id = last_pos, max_new_tokens=TOKEN_PER_IMAGE, windowsize=4)
        else:
            next_frame, last_pos = model.transformer.decode_img_token_for_gradio(input_action=ongoing_act, position_id = last_pos, max_new_tokens=TOKEN_PER_IMAGE + 1) # +1 to fill kvcache
        
    last_pos = last_pos[0]
    next_frame = torch.cat(next_frame, dim=-1).to("cuda")
    frame_cache.append(next_frame)
    next_frame = tokenizer.token2image(next_frame)
    next_frame = Image.fromarray(next_frame)
    if len(images_show) >= SHOW_FRAMES:
        images_show.popleft()
        actions_show.popleft()
    btns_choices = btns_choices + [np.array((int(cam_y_input), int(cam_x_input)))]
    actions_show.append(','.join(str(x) for item in btns_choices for x in (item if isinstance(item, np.ndarray) else [item])))
    images_show.append(next_frame)
    VIDEO_FRAMES.append(next_frame)

    return next_frame, stack_images(images_show), "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;".join([str(x) for x in actions_show])

def run_prediction_n_times(n, btns_1, btns_2, btns_3, btns_4, btns_5, cam_x_input, cam_y_input):
    btns_choices = btns_1 + btns_2 + btns_3 + btns_4 + btns_5
    if cam_x_input is None:
        cam_x_input = 0
    if cam_y_input is None:
        cam_y_input = 0
    if n is None:
        n = 1
    for i in range(n):
        yield run_prediction(btns_choices, cam_x_input, cam_y_input)

def step_pred_source_video_right(video_path, start):
    global VIDEO_FRAMES, frame_cache, action_cache, REFERENCE_FRAME, CONTEXT_LEN, REFERENCE_FRAME
    VIDEO_FRAMES.clear(); frame_cache.clear(); action_cache.clear()
    if start is None or start < 0 or start > MAX_FRAME:
        start = 0
    return step_video(video_path, start, REFERENCE_FRAME)

def on_download_button_click(fps=6):
    if not VIDEO_FRAMES:
        print("The frames list is empty.")
        return

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4", dir="/tmp")
    video_path = temp_file.name
    temp_file.close()

    video_writer = cv2.VideoWriter(
        video_path,
        cv2.VideoWriter_fourcc(*"mp4v"),
        fps,
        AGENT_RESOLUTION
    )

    for frame in VIDEO_FRAMES:
        frame_np = np.array(frame)
        frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)
        video_writer.write(frame_bgr)

    video_writer.release()
    GENERATED_FILES.append(video_path)
    os.chmod(video_path, 0o644)
    return video_path

def cleanup_files():
    for video_path in GENERATED_FILES:
        try:
            os.remove(video_path)
            print(f"Deleted file: {video_path}")
        except OSError as e:
            print(f"Error deleting file {video_path}: {e}")

atexit.register(cleanup_files)

def step_video(video_path, start_frame, frame_count):
    global images_show, actions_show, frame_cache, action_cache, VIDEO_FRAMES, last_pos, CONTEXT_LEN, REFERENCE_FRAME
    VIDEO_FRAMES = []
    images_show = []
    actions_show = []
    video = cv2.VideoCapture(video_path)
    json_data = MC_ACTION_MAP.read_jsonl(video_path[:-4]+".jsonl")

    frames_tensor = []
    action_cache = []
    for i in range(start_frame, start_frame + frame_count):
        step_action = json_data[i]
        step_action, _ = MC_ACTION_MAP.json_action_to_env_action(step_action)
        actions_show.append(get_action_line(step_action))
        
        act_index = MC_ACTION_MAP.get_action_index_from_actiondict(step_action, action_vocab_offset=8192)
        act_index = torch.tensor(act_index).unsqueeze(0)
        action_cache.append(act_index.to("cuda"))

        video.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = video.read()
        try:
            if not ret:
                raise ValueError(f"frame {i} not ret")
            cv2.cvtColor(frame, code=cv2.COLOR_BGR2RGB, dst=frame)

            frame = np.asarray(np.clip(frame, 0, 255), dtype=np.uint8)
            frame = cv2.resize(frame, AGENT_RESOLUTION, interpolation=cv2.INTER_LINEAR)
            images_show.append(Image.fromarray(frame))
            VIDEO_FRAMES.append(Image.fromarray(frame))
            frames_tensor.append(torch.from_numpy(frame))
            
        except Exception as e:
            print(f"Could not read frame from video {video_path}: {e}")
    video.release()
    frames_tensor = torch.stack(frames_tensor, dim=0).to("cuda")
    frames_tensor = frames_tensor.permute(0, 3, 1, 2).float() / 255.0
    frames_tensor = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])(frames_tensor)

    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16):
        images_token = tokenizer.tokenize_images(frames_tensor)
    images_token = rearrange(images_token, '(b t) h w -> (b t) (h w)', b=1)
    
    frame_cache = deque(torch.split(images_token, split_size_or_sections=1, dim=0))
    action_cache = deque(action_cache)
    action_cache.pop()

    images_show = deque(images_show)
    actions_show = deque(actions_show)

    actions_show.pop()

    
    model.transformer.refresh_kvcache()
    _frame_iter = itertools.islice(frame_cache, 0, len(frame_cache)-1)
    _act_iter = itertools.islice(action_cache, 0, len(action_cache))

    
    _vis_act = [
            torch.cat([img, act], dim=1)
            for img, act in zip(_frame_iter, _act_iter)
        ]
    _vis_act.append(frame_cache[-1])
    _vis_act = torch.cat(_vis_act, dim=-1)

    _, last_pos = model.transformer.prefill_for_gradio(_vis_act)

    while len(images_show) > SHOW_FRAMES:
        images_show.popleft()
        actions_show.popleft()
        # WARNING: why dont pop actions
    
    return stack_images(images_show), "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;".join([str(x) for x in actions_show]), None


css = """
.custom-tab h2 { 
    font-size: 34px;  /* 字体大小 */
    font-weight: bold;  /* 加粗字体 */
    color: #ff6600;  /* 字体颜色 */
    text-shadow: 1px 1px 2px #000000; /* 文字阴影效果 */
}
"""

if __name__ == "__main__":
    args = get_args()
    if args.diagd:
        DIAGD = True
        WINDOWSIZE = args.window_size
    cap = cv2.VideoCapture(args.scene)
    global MAX_FRAME
    MAX_FRAME = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 10

    config = OmegaConf.load(args.config)
    REFERENCE_FRAME = args.reference_frame
    CONTEXT_LEN = int(config.model.params.transformer_config.params.max_position_embeddings / (TOKEN_PER_ACTION + TOKEN_PER_IMAGE))
    assert CONTEXT_LEN > REFERENCE_FRAME
    model = load_model(config, args.model_ckpt, gpu=True, eval_mode=True)
    tokenizer = model.tokenizer
    with gr.Blocks(css=css) as demo:
        with gr.Tab("MineWorld", elem_classes="custom-tab"):
            source_video_path = gr.Text(value=args.scene, visible=False)
            with gr.Row():
                source_video_actions = gr.Markdown(visible=False)
                instruction = gr.Markdown("press 'Jump to start frame' to init or restart the game, you can choose different sences by modifed start frame from 0 to 4100", visible=False)
            with gr.Row():
                source_video_images = gr.Image(width=1280, height=360, label="last 8 frames", show_fullscreen_button = True, every=1)

            with gr.Row(equal_height=True):                    
                with gr.Column(min_width=60):
                    vid_frame_start = gr.Number(step=1, value=0, info="start frame", min_width=20, show_label=False, minimum=0, maximum=MAX_FRAME)
                        # vid_num_frames = gr.Number(step=1, value=4, label="num_frames", min_width=50)
                # with gr.Column(min_width=60):
                    run_steps = gr.Number(step=1, value=1, info="Repeat same action n times", min_width=20, minimum=1, maximum=8, show_label=False)
                with gr.Column(min_width=60):
                    btn1 = list(FOR_BACK.keys())
                    btns_1 = gr.CheckboxGroup(choices=btn1, show_label=False)
                    vid_right_btn = gr.Button(value="Jump to start frame", size='sm')
                with gr.Column(min_width=60):
                    btn2 = list(L_R.keys())
                    btns_2 = gr.CheckboxGroup(choices=btn2, show_label=False)
                    predict_run_btn = gr.Button(value="Run", variant="primary", size='sm')
                with gr.Column(min_width=60):
                    btn4 = list(JUMP_SPR.keys())
                    btns_4 = gr.CheckboxGroup(choices=btn4, show_label=False)
                    download_game_btn = gr.Button("Generate Video", size='sm')
                with gr.Column(min_width=60):
                    cam_y_input = gr.Number(step=1, value=0, info="camera Y ⬆️(-),0,(+)⬇️", min_width=20, minimum=-90, maximum=90, show_label=False)
                    cam_x_input = gr.Number(step=1, value=0, info="camera X ⬅️(-),0,(+)➡️", min_width=20, minimum=-90, maximum=90, show_label=False)
                
                    
            with gr.Row():
                with gr.Column(min_width=250):
                    video_display = gr.Video(label="video", width=384, height=224)
                with gr.Column(min_width=200): 
                    predict_result_imgs = gr.Image(label="last generated frame",width=384, height=224)
                with gr.Column(min_width=200):
                    with gr.Row():
                        btn3 = list(ATT_USE_DROP.keys())
                        btns_3 = gr.CheckboxGroup(choices=btn3, show_label=False)
                    with gr.Row():
                        btn5 = list(HORBAR.keys())
                        btns_5 = gr.CheckboxGroup(choices=btn5, show_label=False)

                    
                
            vid_right_btn.click(fn=step_pred_source_video_right, inputs=[source_video_path, vid_frame_start],
                                outputs=[source_video_images, source_video_actions, predict_result_imgs])                
            predict_run_btn.click(fn=run_prediction_n_times, inputs=[run_steps, btns_1, btns_2, btns_3, btns_4, btns_5, cam_x_input, cam_y_input],
                                    outputs=[predict_result_imgs, source_video_images, source_video_actions],)
            download_game_btn.click(fn=on_download_button_click, inputs=[], outputs=video_display)
            demo.load(fn=step_pred_source_video_right, inputs=[source_video_path, gr.Number(value=25, visible=False)],
                                outputs=[source_video_images, source_video_actions, predict_result_imgs])
    demo.queue()
    
    demo.launch(server_name="0.0.0.0", max_threads=256, server_port=7861, share=True)